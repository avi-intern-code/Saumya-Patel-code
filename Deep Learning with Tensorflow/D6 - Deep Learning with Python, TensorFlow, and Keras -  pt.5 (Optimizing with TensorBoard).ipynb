{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [0,1,2]\n",
    "layer_sizes = [32, 64, 128]\n",
    "conv_layers = [1, 2, 3]\n",
    "\n",
    "# modify layers and nodes per layer, as well as 0, 1, or 2 dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(X/255)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-conv-32-nodes-0-dense-1621676186\n",
      "2-conv-32-nodes-0-dense-1621676190\n",
      "3-conv-32-nodes-0-dense-1621676190\n",
      "1-conv-64-nodes-0-dense-1621676191\n",
      "2-conv-64-nodes-0-dense-1621676191\n",
      "3-conv-64-nodes-0-dense-1621676191\n",
      "1-conv-128-nodes-0-dense-1621676191\n",
      "2-conv-128-nodes-0-dense-1621676191\n",
      "3-conv-128-nodes-0-dense-1621676191\n",
      "1-conv-32-nodes-1-dense-1621676192\n",
      "2-conv-32-nodes-1-dense-1621676192\n",
      "3-conv-32-nodes-1-dense-1621676192\n",
      "1-conv-64-nodes-1-dense-1621676192\n",
      "2-conv-64-nodes-1-dense-1621676192\n",
      "3-conv-64-nodes-1-dense-1621676193\n",
      "1-conv-128-nodes-1-dense-1621676193\n",
      "2-conv-128-nodes-1-dense-1621676194\n",
      "3-conv-128-nodes-1-dense-1621676194\n",
      "1-conv-32-nodes-2-dense-1621676194\n",
      "2-conv-32-nodes-2-dense-1621676195\n",
      "3-conv-32-nodes-2-dense-1621676195\n",
      "1-conv-64-nodes-2-dense-1621676195\n",
      "2-conv-64-nodes-2-dense-1621676195\n",
      "3-conv-64-nodes-2-dense-1621676196\n",
      "1-conv-128-nodes-2-dense-1621676196\n",
      "2-conv-128-nodes-2-dense-1621676197\n",
      "3-conv-128-nodes-2-dense-1621676197\n"
     ]
    }
   ],
   "source": [
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(NAME)\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(layer_size, (3, 3), input_shape=X.shape[1:]))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(layer_size, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            model.add(Flatten())\n",
    "            for _ in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-conv-32-nodes-0-dense-1621676215\n",
      "Epoch 1/10\n",
      "546/546 [==============================] - 109s 191ms/step - loss: 0.6222 - accuracy: 0.6531 - val_loss: 0.5906 - val_accuracy: 0.6772\n",
      "Epoch 2/10\n",
      "546/546 [==============================] - 101s 186ms/step - loss: 0.5293 - accuracy: 0.7397 - val_loss: 0.5497 - val_accuracy: 0.7223\n",
      "Epoch 3/10\n",
      "546/546 [==============================] - 99s 182ms/step - loss: 0.4859 - accuracy: 0.7695 - val_loss: 0.5353 - val_accuracy: 0.7361\n",
      "Epoch 4/10\n",
      "546/546 [==============================] - 96s 176ms/step - loss: 0.4402 - accuracy: 0.7993 - val_loss: 0.5367 - val_accuracy: 0.7348\n",
      "Epoch 5/10\n",
      "546/546 [==============================] - 99s 181ms/step - loss: 0.4072 - accuracy: 0.8203 - val_loss: 0.5549 - val_accuracy: 0.7378\n",
      "Epoch 6/10\n",
      "546/546 [==============================] - 103s 188ms/step - loss: 0.3702 - accuracy: 0.8364 - val_loss: 0.5392 - val_accuracy: 0.7450\n",
      "Epoch 7/10\n",
      "546/546 [==============================] - 98s 179ms/step - loss: 0.3394 - accuracy: 0.8550 - val_loss: 0.5752 - val_accuracy: 0.7270\n",
      "Epoch 8/10\n",
      "546/546 [==============================] - 99s 181ms/step - loss: 0.3106 - accuracy: 0.8701 - val_loss: 0.5900 - val_accuracy: 0.7327\n",
      "Epoch 9/10\n",
      "546/546 [==============================] - 99s 181ms/step - loss: 0.2881 - accuracy: 0.8856 - val_loss: 0.5878 - val_accuracy: 0.7385\n",
      "Epoch 10/10\n",
      "546/546 [==============================] - 97s 177ms/step - loss: 0.2644 - accuracy: 0.8974 - val_loss: 0.6224 - val_accuracy: 0.7310\n",
      "2-conv-32-nodes-0-dense-1621677221\n",
      "Epoch 1/10\n",
      "546/546 [==============================] - 195s 351ms/step - loss: 0.6367 - accuracy: 0.6266 - val_loss: 0.5627 - val_accuracy: 0.7101\n",
      "Epoch 2/10\n",
      "546/546 [==============================] - 181s 331ms/step - loss: 0.5327 - accuracy: 0.7330 - val_loss: 0.5089 - val_accuracy: 0.7549\n",
      "Epoch 3/10\n",
      "546/546 [==============================] - 183s 335ms/step - loss: 0.4930 - accuracy: 0.7626 - val_loss: 0.4867 - val_accuracy: 0.7696\n",
      "Epoch 4/10\n",
      "546/546 [==============================] - 178s 327ms/step - loss: 0.4537 - accuracy: 0.7920 - val_loss: 0.4818 - val_accuracy: 0.7731\n",
      "Epoch 5/10\n",
      "546/546 [==============================] - 171s 313ms/step - loss: 0.4237 - accuracy: 0.8059 - val_loss: 0.4732 - val_accuracy: 0.7825\n",
      "Epoch 6/10\n",
      "546/546 [==============================] - 117s 214ms/step - loss: 0.3970 - accuracy: 0.8189 - val_loss: 0.4715 - val_accuracy: 0.7862\n",
      "Epoch 7/10\n",
      "546/546 [==============================] - 79s 145ms/step - loss: 0.3690 - accuracy: 0.8370 - val_loss: 0.4620 - val_accuracy: 0.7908\n",
      "Epoch 8/10\n",
      "546/546 [==============================] - 82s 150ms/step - loss: 0.3443 - accuracy: 0.8519 - val_loss: 0.4697 - val_accuracy: 0.7916\n",
      "Epoch 9/10\n",
      "546/546 [==============================] - 79s 145ms/step - loss: 0.3200 - accuracy: 0.8632 - val_loss: 0.4696 - val_accuracy: 0.7929\n",
      "Epoch 10/10\n",
      "546/546 [==============================] - 82s 150ms/step - loss: 0.2982 - accuracy: 0.8735 - val_loss: 0.4858 - val_accuracy: 0.7920\n",
      "3-conv-32-nodes-0-dense-1621678579\n",
      "Epoch 1/10\n",
      "546/546 [==============================] - 95s 172ms/step - loss: 0.6373 - accuracy: 0.6300 - val_loss: 0.5741 - val_accuracy: 0.7077\n",
      "Epoch 2/10\n",
      "546/546 [==============================] - 93s 171ms/step - loss: 0.5356 - accuracy: 0.7349 - val_loss: 0.4988 - val_accuracy: 0.7611\n",
      "Epoch 3/10\n",
      "546/546 [==============================] - 96s 175ms/step - loss: 0.4841 - accuracy: 0.7705 - val_loss: 0.4888 - val_accuracy: 0.7711\n",
      "Epoch 4/10\n",
      "546/546 [==============================] - 90s 165ms/step - loss: 0.4439 - accuracy: 0.7919 - val_loss: 0.5209 - val_accuracy: 0.7464\n",
      "Epoch 5/10\n",
      "546/546 [==============================] - 91s 168ms/step - loss: 0.4158 - accuracy: 0.8100 - val_loss: 0.4429 - val_accuracy: 0.7934\n",
      "Epoch 6/10\n",
      "546/546 [==============================] - 92s 169ms/step - loss: 0.3866 - accuracy: 0.8254 - val_loss: 0.4120 - val_accuracy: 0.8159\n",
      "Epoch 7/10\n",
      "546/546 [==============================] - 90s 165ms/step - loss: 0.3739 - accuracy: 0.8311 - val_loss: 0.3914 - val_accuracy: 0.8262\n",
      "Epoch 8/10\n",
      "546/546 [==============================] - 90s 164ms/step - loss: 0.3456 - accuracy: 0.8459 - val_loss: 0.4069 - val_accuracy: 0.8155\n",
      "Epoch 9/10\n",
      "546/546 [==============================] - 89s 163ms/step - loss: 0.3256 - accuracy: 0.8604 - val_loss: 0.4135 - val_accuracy: 0.8238\n",
      "Epoch 10/10\n",
      "546/546 [==============================] - 87s 160ms/step - loss: 0.3030 - accuracy: 0.8672 - val_loss: 0.3683 - val_accuracy: 0.8416\n",
      "1-conv-64-nodes-0-dense-1621679498\n",
      "Epoch 1/10\n",
      "546/546 [==============================] - 95s 172ms/step - loss: 0.6174 - accuracy: 0.6599 - val_loss: 0.5630 - val_accuracy: 0.7074\n",
      "Epoch 2/10\n",
      "546/546 [==============================] - 88s 161ms/step - loss: 0.5234 - accuracy: 0.7450 - val_loss: 0.5444 - val_accuracy: 0.7241\n",
      "Epoch 3/10\n",
      "546/546 [==============================] - 88s 162ms/step - loss: 0.4678 - accuracy: 0.7828 - val_loss: 0.5456 - val_accuracy: 0.7295\n",
      "Epoch 4/10\n",
      "546/546 [==============================] - 89s 163ms/step - loss: 0.4234 - accuracy: 0.8080 - val_loss: 0.5826 - val_accuracy: 0.7128\n",
      "Epoch 5/10\n",
      "546/546 [==============================] - 89s 163ms/step - loss: 0.3891 - accuracy: 0.8301 - val_loss: 0.5527 - val_accuracy: 0.7339\n",
      "Epoch 6/10\n",
      "546/546 [==============================] - 91s 167ms/step - loss: 0.3420 - accuracy: 0.8547 - val_loss: 0.5668 - val_accuracy: 0.7413\n",
      "Epoch 7/10\n",
      "546/546 [==============================] - 102s 187ms/step - loss: 0.3051 - accuracy: 0.8713 - val_loss: 0.5880 - val_accuracy: 0.7389\n",
      "Epoch 8/10\n",
      "546/546 [==============================] - 100s 184ms/step - loss: 0.2746 - accuracy: 0.8911 - val_loss: 0.6087 - val_accuracy: 0.7314\n",
      "Epoch 9/10\n",
      "546/546 [==============================] - 100s 183ms/step - loss: 0.2371 - accuracy: 0.9092 - val_loss: 0.6550 - val_accuracy: 0.7279\n",
      "Epoch 10/10\n",
      "546/546 [==============================] - 100s 182ms/step - loss: 0.2102 - accuracy: 0.9227 - val_loss: 0.6725 - val_accuracy: 0.7275\n",
      "2-conv-64-nodes-0-dense-1621680442\n",
      "Epoch 1/10\n",
      "546/546 [==============================] - 199s 356ms/step - loss: 0.6535 - accuracy: 0.6124 - val_loss: 0.6398 - val_accuracy: 0.6363\n",
      "Epoch 2/10\n",
      "546/546 [==============================] - 206s 377ms/step - loss: 0.5610 - accuracy: 0.7115 - val_loss: 0.5350 - val_accuracy: 0.7320\n",
      "Epoch 3/10\n",
      "546/546 [==============================] - 191s 350ms/step - loss: 0.5021 - accuracy: 0.7548 - val_loss: 0.5077 - val_accuracy: 0.7488\n",
      "Epoch 4/10\n",
      "546/546 [==============================] - 196s 360ms/step - loss: 0.4616 - accuracy: 0.7854 - val_loss: 0.4940 - val_accuracy: 0.7654\n",
      "Epoch 5/10\n",
      "337/546 [=================>............] - ETA: 1:11 - loss: 0.4295 - accuracy: 0.8026"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[32,98,98,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node gradient_tape/sequential_31/max_pooling2d_61/MaxPool/MaxPoolGrad (defined at <ipython-input-10-ab402187391f>:34) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_72631]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ab402187391f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m                           )\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             model.fit(X, y,\n\u001b[0m\u001b[0;32m     35\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[32,98,98,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node gradient_tape/sequential_31/max_pooling2d_61/MaxPool/MaxPoolGrad (defined at <ipython-input-10-ab402187391f>:34) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_72631]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(NAME)\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(layer_size, (3, 3), input_shape=X.shape[1:]))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(layer_size, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            model.add(Flatten())\n",
    "\n",
    "            for _ in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'],\n",
    "                          )\n",
    "\n",
    "            model.fit(X, y,\n",
    "                      batch_size=32,\n",
    "                      epochs=10,\n",
    "                      validation_split=0.3,\n",
    "                      callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
